<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
    <title>
Scraping JavaScript-Based Dynamic Pages &middot; siwu&#x27;s blog
</title>
    <link rel="stylesheet" href="https://gaxxx.github.io/slim.css">
    
    
</head>

<body>
    <div class="container">
        
<div class="header">
    <h1 class="site-title"><a href="https:&#x2F;&#x2F;gaxxx.github.io">siwu&#x27;s blog</a></h1>
    <input id="search" class=".next" type="search" placeholder="Search" />
    <p class="site-tagline"></p>
    <div class="nav">
        <a class="nav-btn" href="#">
            <span class="ci ci-burger"></span>
        </a>
        <ul class="nav-list">
            
            
            <li><a href="https:&#x2F;&#x2F;gaxxx.github.io&#x2F;tags">Tags</a></li>
            
            
            <li class="spacer">&ac;</li>
            
            
            <li><a href="https://www.linkedin.com/in/gaxxx">LinkedIn</a></li>
            
            <li><a href="https://github.com/gaxxx">Github</a></li>
            
            
        </ul>
    </div>
    <div class="search-results">
    <ul class="search-results__items"></ul>
    </div>

</div>

        <div class="content">
            <div class="posts">
                
<div class="post">
    
    
<h2 class="post-title"><a href="https:&#x2F;&#x2F;gaxxx.github.io&#x2F;zhua-qu-ji-yu-jsde-dong-tai-ye-mian&#x2F;">Scraping JavaScript-Based Dynamic Pages</a></h2>
<div class="post-header">
    <span class="post-date">September 21, 2014</span>
    
    <div class="post-tags">
        
        <span class="post-tag">#<a href="https://gaxxx.github.io/tags/selenum/">selenum</a></span>
        
    </div>
    
</div>

    

  
    
    <a class="lang-toggle" href="https:&#x2F;&#x2F;gaxxx.github.io&#x2F;zh&#x2F;zhua-qu-ji-yu-jsde-dong-tai-ye-mian&#x2F;">
      中文
    </a>
    
  
    
  


    <div class="post-content">
        <p>Recently worked on a project that required scraping dynamic web page content. Accumulated some experience, here's a brief summary of web scraping techniques.</p>
<span id="continue-reading"></span><h1 id="system-environment">System Environment</h1>
<ul>
<li>OS: ArchLinux@64bits</li>
<li>Packages: python2-pip and pip-installed selenium, beautifulsoup4</li>
</ul>
<h1 id="fetching-static-content">Fetching Static Content</h1>
<p>Fetching static content is relatively simple - just ensure cookies are correct.</p>
<h2 id="direct-fetch">Direct Fetch</h2>
<pre><code>import urllib2
r = urllib2.urlopen(&quot;http://www.baidu.com/&quot;)
print r.read()
</code></pre>
<h2 id="fetching-static-content-that-requires-login">Fetching Static Content That Requires Login</h2>
<pre><code>import urllib2,urllib
cookie = urllib2.HTTPCookieProcessor()
opener = urllib2.build_opener(cookie)
## Construct login form data
user_pass = [(&quot;email&quot;,&quot;abc@example.com&quot;),(&quot;password&quot;,&quot;lab&quot;),(&#39;op&#39;,&#39;Log in&#39;)]
r = opener.open(&quot;http://example.com/index.php?&quot;,urllib.urlencode(user_pass))
print r.read()
</code></pre>
<h2 id="fetching-static-content-with-openid-login">Fetching Static Content with OpenID Login</h2>
<pre><code>#Some sites require OpenID login. For OAuth protocol, first log in normally in Chrome, then export cookies to cookie.txt using the &quot;cookie.txt export&quot; extension
import urllib2,urllib,cookielib
cj = cookielib.FileCookieJar(&quot;./cookie.txt&quot;)
cookie = urllib2.HTTPCookieProcessor(cj)
opener = urllib2.build_opener(cookie)
r = opener.open(&quot;http://example.com/&quot;)
print r.read()
</code></pre>
<h1 id="reading-dynamically-generated-content">Reading Dynamically Generated Content</h1>
<p>For dynamically generated content (mainly pages generated via JavaScript), here are some approaches from Zhihu:</p>
<ul>
<li>Use phantomjs &amp; casperjs - call JS code, JS calling JS, seamless integration...</li>
<li>Use selenium - drive browsers for rendering and content extraction</li>
<li>Use scrapy framework with pyV8 for large-scale scraping (Etao's approach)</li>
</ul>
<h2 id="scraping-with-phantomjs">Scraping with PhantomJS</h2>
<pre><code>#Install phantomjs
#test.js file
var page = require(&#39;webpage&#39;).create();
page.onResourceReceived = function(data) {
    console.log(&quot;Got a url:&quot; + data.url);
    console.log(&quot;Got a type:&quot; + data.contentType);
};
page.open(&#39;http://www.baidu.com&#39;,function(status){
        phantom.exit()
})

#phantomjs test.js
</code></pre>
<p>PhantomJS has an interesting feature - it supports website screenshots, rendering pages to PNG. However, PhantomJS doesn't support file downloads. There's a download_support branch worth trying.</p>
<h2 id="downloading-data-with-selenium">Downloading Data with Selenium</h2>
<p>The advantage of selenium is that anything a browser can do, a selenium script can do too. Common usage is automating button clicks and testing page responses, but here we use it for downloading.</p>
<pre><code>from selenium import webdriver
import glob
import time
options = webdriver.ChromeOptions()
#Create an empty Chrome profile directory
options.add_argument(&#39;--user-data-dir=./chrome&#39;)
browser = webdriver.Chrome(chrome_options=options)
#Start downloading a file - opens a Chromium browser where you can log in, install extensions, then restart the script for normal downloading
browser.get(&quot;http://example.com/test.mp4&quot;)
#Check if file is downloaded by file extension
while True：
    //File downloading
    files = glob.glob(os.path.join(&quot;xxxx/Downloads&quot;,&quot;*.crdownload&quot;))
    if len(files) == 0:
        time.sleep(0.1)
    else:
        break

//File download complete
while True:
    files = glob.glob(os.path.join(&quot;xxxx/Downloads&quot;,&quot;*.crdownload&quot;))
    if len(files) &gt; 0:
        time.sleep(0.1)
    else:
        break

//Move file to another location
try:
    out = glob.glob(os.path.join(&quot;xxxx/Downloads&quot;,&quot;*.mp4&quot;))[0]
    shuitl.move(out,&quot;/tmp/test.mp4&quot;)
except:
    print &quot;failed to get %s&quot; % out
</code></pre>
<h2 id="scraping-with-scrapy-todo">Scraping with Scrapy (TODO)</h2>
<h1 id="appendix-beautiful-soup">Appendix: Beautiful Soup</h1>
<p>Beautiful Soup is one of Python's most useful frameworks for parsing static HTML structure and finding relevant content. Common usage:</p>
<pre><code>#Get Youku&#39;s featured videos
from bs4 import BeautifulSoup
import urllib2
r = urllib2.urlopen(&quot;http://www.youku.com/&quot;)
soup = BeautifulSoup(r.read())
r.close()
#Print featured video title
print soup.find(&quot;div&quot;,&quot;v-thumb&quot;).find(&quot;img&quot;).get(&quot;alt&quot;)
</code></pre>

    </div>
    
</div>

            </div>
            
        </div>
        
<div class="footer">
    
    <p>Powered by <a href="https://getzola.org">Zola</a></p>
    
</div>

    </div>
    <script src="https://gaxxx.github.io/js/slim.js"></script>
    <script src="https://gaxxx.github.io/js/elasticlunr.min.js"></script>
    <script src="https://gaxxx.github.io/search_index.en.js"></script>
    <script src="https://gaxxx.github.io/js/lunr.stemmer.support.js"></script>
    <script src="https://gaxxx.github.io/js/lunr.en.js"></script>
    <script src="https://gaxxx.github.io/js/search.js"></script>
    
</body>

</html>
